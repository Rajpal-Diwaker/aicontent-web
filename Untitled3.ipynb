{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'float' object has no attribute 'strip'\n",
      "'float' object has no attribute 'strip'\n",
      "'float' object has no attribute 'strip'\n",
      "'float' object has no attribute 'strip'\n",
      "'float' object has no attribute 'strip'\n",
      "'float' object has no attribute 'strip'\n",
      "'float' object has no attribute 'strip'\n",
      "'float' object has no attribute 'strip'\n",
      "'float' object has no attribute 'strip'\n",
      "'float' object has no attribute 'strip'\n",
      "list index out of range\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "from html.parser import HTMLParser\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "porter = PorterStemmer()\n",
    "wnl = WordNetLemmatizer() \n",
    "stop = stopwords.words('english')\n",
    "stop = set(stop)\n",
    "\n",
    "def tokenizer(text):\n",
    "\n",
    "    tokens_ = [word_tokenize(sent) for sent in sent_tokenize(text)]\n",
    "\n",
    "    tokens = []\n",
    "    for token_by_sent in tokens_:\n",
    "        tokens += token_by_sent\n",
    "\n",
    "    tokens = list(filter(lambda t: t.lower() not in stop, tokens))\n",
    "    tokens = list(filter(lambda t: t not in punctuation, tokens))\n",
    "    tokens = list(filter(lambda t: t not in [u\"'s\", u\"n't\", u\"...\", u\"''\", u'``', u'\\u2014', u'\\u2026', u'\\u2013'], tokens))\n",
    "     \n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        token = wnl.lemmatize(token)\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "\n",
    "    filtered_tokens = list(map(lambda token: token.lower(), filtered_tokens))\n",
    "\n",
    "    return filtered_tokens\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reset()\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "def get_keywords(tokens, num):\n",
    "    return Counter(tokens).most_common(num)\n",
    "\n",
    "def build_article_df(urls):\n",
    "    articles = []\n",
    "    for index, row in urls.iterrows():\n",
    "        try:\n",
    "            data = row['text'].strip().replace(\"'\", \"\")\n",
    "            data = strip_tags(data)\n",
    "            soup = BeautifulSoup(data)\n",
    "            data = soup.get_text()\n",
    "            data = data.encode('ascii', 'ignore').decode('ascii')\n",
    "            document = tokenizer(data)\n",
    "            top_5 = get_keywords(document, 5)\n",
    "          \n",
    "            unzipped = list(zip(*top_5))\n",
    "            kw= list(unzipped[0])\n",
    "            kw=\",\".join(str(x) for x in kw)\n",
    "            articles.append((kw, row['title'], row['pubdate']))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            #print data\n",
    "            #break\n",
    "            pass\n",
    "        #break\n",
    "    article_df = pd.DataFrame(articles, columns=['keywords', 'title', 'pubdate'])\n",
    "    return article_df\n",
    "\n",
    "\n",
    "df = pd.read_csv('./data-model/news.csv')\n",
    "data = []\n",
    "for index, row in df.iterrows():\n",
    "    data.append((row['title'], row['url'], row['publishedAt'], row['description']))\n",
    "data_df = pd.DataFrame(data, columns=['title' ,'url', 'pubdate', 'text' ])\n",
    "\n",
    "article_df = build_article_df(data_df)\n",
    "\n",
    "f = open('data-model/scrapped_df_other.pkl', 'wb')\n",
    "pickle.dump(article_df, f)\n",
    "f.close()\n",
    "\n",
    "# keywords_array=[]\n",
    "# for index, row in article_df.iterrows():\n",
    "#     keywords=row['keywords'].split(',')\n",
    "#     for kw in keywords:\n",
    "#         keywords_array.append((kw.strip(' '), row['keywords']))\n",
    "# kw_df = pd.DataFrame(keywords_array).rename(columns={0:'keyword', 1:'keywords'})\n",
    "\n",
    "# document = kw_df.keywords.tolist()\n",
    "# names = kw_df.keyword.tolist()\n",
    "\n",
    "# document_array = []\n",
    "# for item in document:\n",
    "#     items = item.split(',')\n",
    "#     document_array.append((items))\n",
    "\n",
    "# occurrences = OrderedDict((name, OrderedDict((name, 0) for name in names)) for name in names)\n",
    "\n",
    "# for l in document_array:\n",
    "#     for i in range(len(l)):\n",
    "#         for item in l[:i] + l[i + 1:]:\n",
    "#             occurrences[l[i]][item] += 1\n",
    "\n",
    "# co_occur = pd.DataFrame.from_dict(occurrences )\n",
    "\n",
    "# co_occur.to_csv('data-model/co-occurency-matrix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
