{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk;\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['news', 'top', 'policy', 'post', 'long', 'first', 'told', 'uploads', 'root', 'year', 'summary', 'back', 'name', 'span', 'data', 'new', 'time', 'one', 'last', 'jpg', 'img', 'png', 'html', 'width', 'content', 'text', 'two', 'video', 'source', 'paragraph', 'true', 'style', 'title', 'but', 'height', 'div', 'strong', 'image', 'com', 'href', 'class', 'https', 'www', 'amp', 'said', 'would', 'http', 'src', 'target', 'blank', 'get', 'like', 'blank', 'from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come'])\n",
    "\n",
    "import pickle\n",
    "pickle_in = open(\"data-model/scrapped_df_other.pkl\",\"rb\") \n",
    "df = pickle.load(pickle_in) \n",
    "data = df['description']\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.039*\"politically\" + 0.030*\"freeze\" + 0.020*\"hunt\" + 0.012*\"refinitiv\" + '\n",
      "  '0.012*\"mexican\" + 0.009*\"batter\" + 0.009*\"commercial\" + 0.008*\"interfere\" + '\n",
      "  '0.008*\"obstruction\" + 0.008*\"dealing\"'),\n",
      " (1,\n",
      "  '0.044*\"studio\" + 0.028*\"opportunity\" + 0.024*\"husband\" + 0.022*\"tournament\" '\n",
      "  '+ 0.021*\"graphic\" + 0.014*\"activehow\" + 0.012*\"tennis\" + 0.010*\"contrast\" + '\n",
      "  '0.008*\"australian_open\" + 0.007*\"persistent\"'),\n",
      " (2,\n",
      "  '0.127*\"delivery\" + 0.011*\"portrait\" + 0.009*\"notoriously\" + 0.004*\"utmost\" '\n",
      "  '+ 0.002*\"worthwhile\" + 0.002*\"quiz\" + 0.002*\"secretary\" + 0.001*\"et_fri\" + '\n",
      "  '0.001*\"clare_foran\" + 0.000*\"nowhere\"'),\n",
      " (3,\n",
      "  '0.041*\"virus\" + 0.033*\"people\" + 0.031*\"spread\" + 0.027*\"chinese\" + '\n",
      "  '0.023*\"case\" + 0.021*\"outbreak\" + 0.019*\"travel\" + 0.018*\"report\" + '\n",
      "  '0.015*\"city\" + 0.015*\"authority\"'),\n",
      " (4,\n",
      "  '0.016*\"mess\" + 0.015*\"shit\" + 0.007*\"keyword\" + 0.004*\"whitewash\" + '\n",
      "  '0.003*\"museum\" + 0.003*\"branch\" + 0.002*\"location\" + 0.001*\"nickiminaj\" + '\n",
      "  '0.001*\"dam\" + 0.001*\"tussaud\"'),\n",
      " (5,\n",
      "  '0.077*\"witness\" + 0.057*\"senator\" + 0.043*\"trial\" + 0.039*\"vote\" + '\n",
      "  '0.032*\"manager\" + 0.028*\"impeachment\" + 0.027*\"house\" + 0.018*\"adviser\" + '\n",
      "  '0.012*\"impeachment_manager\" + 0.012*\"seat\"'),\n",
      " (6,\n",
      "  '0.018*\"company\" + 0.010*\"use\" + 0.009*\"make\" + 0.009*\"service\" + '\n",
      "  '0.008*\"work\" + 0.007*\"include\" + 0.007*\"system\" + 0.006*\"build\" + '\n",
      "  '0.006*\"say\" + 0.006*\"change\"'),\n",
      " (7,\n",
      "  '0.030*\"kill\" + 0.030*\"crash\" + 0.030*\"fire\" + 0.030*\"flight\" + '\n",
      "  '0.028*\"attack\" + 0.023*\"fly\" + 0.018*\"iraqi\" + 0.013*\"area\" + '\n",
      "  '0.013*\"accord\" + 0.013*\"plane\"'),\n",
      " (8,\n",
      "  '0.025*\"car\" + 0.019*\"food\" + 0.015*\"music\" + 0.015*\"people\" + 0.014*\"stage\" '\n",
      "  '+ 0.013*\"award\" + 0.013*\"life\" + 0.013*\"song\" + 0.010*\"sense\" + '\n",
      "  '0.009*\"speaker\"'),\n",
      " (9,\n",
      "  '0.017*\"pilot\" + 0.016*\"ban\" + 0.014*\"man\" + 0.014*\"accord\" + 0.014*\"use\" + '\n",
      "  '0.011*\"student\" + 0.010*\"shoot\" + 0.010*\"speech\" + 0.009*\"weather\" + '\n",
      "  '0.009*\"follow\"'),\n",
      " (10,\n",
      "  '0.039*\"tv\" + 0.034*\"regulator\" + 0.034*\"subscriber\" + 0.028*\"streaming\" + '\n",
      "  '0.028*\"ipad\" + 0.027*\"stream\" + 0.023*\"show\" + 0.022*\"ad\" + 0.019*\"captain\" '\n",
      "  '+ 0.015*\"smash\"'),\n",
      " (11,\n",
      "  '0.064*\"game\" + 0.024*\"window\" + 0.019*\"console\" + 0.019*\"update\" + '\n",
      "  '0.018*\"developer\" + 0.018*\"provide\" + 0.014*\"gaming\" + 0.014*\"link\" + '\n",
      "  '0.014*\"release\" + 0.013*\"port\"'),\n",
      " (12,\n",
      "  '0.015*\"phone\" + 0.013*\"original\" + 0.012*\"draft\" + 0.011*\"announce\" + '\n",
      "  '0.011*\"file\" + 0.011*\"feed\" + 0.010*\"cell\" + 0.010*\"send\" + 0.010*\"leak\" + '\n",
      "  '0.009*\"hack\"'),\n",
      " (13,\n",
      "  '0.024*\"least_minute\" + 0.024*\"inboxget\" + 0.024*\"cnbc_delivere\" + '\n",
      "  '0.024*\"snapshot\" + 0.022*\"info\" + 0.021*\"log\" + 0.019*\"revolution\" + '\n",
      "  '0.017*\"dip\" + 0.015*\"factory\" + 0.014*\"delay\"'),\n",
      " (14,\n",
      "  '0.044*\"market\" + 0.039*\"company\" + 0.021*\"employee\" + 0.017*\"global\" + '\n",
      "  '0.015*\"business\" + 0.014*\"rise\" + 0.013*\"price\" + 0.013*\"investor\" + '\n",
      "  '0.012*\"economy\" + 0.012*\"high\"'),\n",
      " (15,\n",
      "  '0.012*\"report\" + 0.011*\"government\" + 0.010*\"people\" + 0.009*\"country\" + '\n",
      "  '0.008*\"issue\" + 0.008*\"say\" + 0.008*\"world\" + 0.006*\"call\" + 0.006*\"leader\" '\n",
      "  '+ 0.006*\"part\"'),\n",
      " (16,\n",
      "  '0.021*\"people\" + 0.019*\"story\" + 0.015*\"family\" + 0.013*\"show\" + '\n",
      "  '0.012*\"post\" + 0.011*\"find\" + 0.010*\"old\" + 0.010*\"woman\" + 0.009*\"life\" + '\n",
      "  '0.009*\"go\"'),\n",
      " (17,\n",
      "  '0.054*\"abortion\" + 0.025*\"percent\" + 0.024*\"doctor\" + 0.021*\"epidemic\" + '\n",
      "  '0.014*\"deadline\" + 0.011*\"nonetheless\" + 0.009*\"exit\" + 0.008*\"doom\" + '\n",
      "  '0.008*\"diagnose\" + 0.007*\"demographic\"'),\n",
      " (18,\n",
      "  '0.015*\"go\" + 0.014*\"game\" + 0.013*\"get\" + 0.013*\"player\" + 0.013*\"play\" + '\n",
      "  '0.012*\"team\" + 0.010*\"season\" + 0.009*\"give\" + 0.009*\"look\" + '\n",
      "  '0.008*\"point\"'),\n",
      " (19,\n",
      "  '0.039*\"trump\" + 0.018*\"investigation\" + 0.014*\"case\" + 0.013*\"president\" + '\n",
      "  '0.013*\"call\" + 0.012*\"lawyer\" + 0.012*\"former\" + 0.010*\"legal\" + '\n",
      "  '0.010*\"impeachment\" + 0.010*\"charge\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -11.422166449154608\n",
      "\n",
      "Coherence Score:  0.463618454866008\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file created\n"
     ]
    }
   ],
   "source": [
    "def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(lda_model, corpus=corpus, texts=data)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "import os\n",
    "filenamee = './data-model/df_dominant_topic.pkl'\n",
    "\n",
    "with open(filenamee, 'wb') as fout:\n",
    "    pickle.dump(df_dominant_topic, fout)\n",
    "    print(\"file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file created\n"
     ]
    }
   ],
   "source": [
    "# Group top 5 sentences under each topic\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "import os\n",
    "filename = './data-model/sent_topics_sorteddf_mallet.pkl'\n",
    "\n",
    "with open(filename, 'wb') as fout:\n",
    "    pickle.dump(sent_topics_sorteddf_mallet, fout)\n",
    "    print(\"file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
