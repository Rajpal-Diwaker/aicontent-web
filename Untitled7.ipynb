{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7924a7e1f6d047c287de9773794c287d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arjun/anaconda3/lib/python3.7/site-packages/tqdm/std.py:648: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Int64Index([0], dtype='int64')] are in the [index]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1a6d3031ab6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'description'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'description'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1424\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1837\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot index with multidimensional key\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1839\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m             \u001b[0;31m# nested tuple slicing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m             \u001b[0;31m# A collection of keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m             return self.obj._reindex_with_indexers(\n\u001b[1;32m   1135\u001b[0m                 \u001b[0;34m{\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m         self._validate_read_indexer(\n\u001b[0;32m-> 1092\u001b[0;31m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1093\u001b[0m         )\n\u001b[1;32m   1094\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 raise KeyError(\n\u001b[1;32m   1176\u001b[0m                     \"None of [{key}] are in the [{axis}]\".format(\n\u001b[0;32m-> 1177\u001b[0;31m                         \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1178\u001b[0m                     )\n\u001b[1;32m   1179\u001b[0m                 )\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Int64Index([0], dtype='int64')] are in the [index]\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = 200\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "from string import punctuation\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "tqdm_notebook().pandas()\n",
    "\n",
    "pickle_in = open(\"data-model/scrapped_df_other.pkl\",\"rb\") \n",
    "data = pickle.load(pickle_in) \n",
    "\n",
    "data = data.drop_duplicates('description')\n",
    "data = data[~data['description'].isnull()]\n",
    "data = data[(data.description.map(len) > 140) & (data.description.map(len) <= 300)]\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "stop_words = []\n",
    "\n",
    "f = open('./data-model/stopwords.txt', 'r')\n",
    "for l in f.readlines():\n",
    "    stop_words.append(l.replace('\\n', ''))\n",
    "    \n",
    "additional_stop_words = ['t', 'will']\n",
    "stop_words += additional_stop_words\n",
    "\n",
    "def reduce(function, iterable, initializer=None):\n",
    "    it = iter(iterable)\n",
    "    if initializer is None:\n",
    "        value = next(it)\n",
    "    else:\n",
    "        value = initializer\n",
    "    for element in it:\n",
    "        value = function(value, element)\n",
    "    return value\n",
    "\n",
    "def _removeNonAscii(s): \n",
    "    return \"\".join(i for i in s if ord(i)<128)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = text.replace('(ap)', '')\n",
    "    text = re.sub(r\"\\'s\", \" is \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r\"\\\\\", \"\", text)\n",
    "    text = re.sub(r\"\\'\", \"\", text)    \n",
    "    text = re.sub(r\"\\\"\", \"\", text)\n",
    "    text = re.sub('[^a-zA-Z ?!]+', '', text)\n",
    "    text = _removeNonAscii(text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = clean_text(text)    \n",
    "    tokens = [word_tokenize(sent) for sent in sent_tokenize(text)]\n",
    "    tokens = list(reduce(lambda x,y: x+y, tokens))\n",
    "    tokens = list(filter(lambda token: token not in (stop_words + list(punctuation)) , tokens))\n",
    "    return tokens\n",
    "\n",
    "data = data.loc[[0]]\n",
    "\n",
    "data['description'] = data['description'].map(lambda d: str(d))\n",
    "data['tokens'] = data['description'].progress_map(lambda d: tokenizer(d))\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim import matutils\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "aux = data.copy()\n",
    "\n",
    "bigram = gensim.models.Phrases(aux['tokens'], min_count=5, threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "aux['tokens_bigram'] = aux['tokens'].progress_map(lambda tokens: bigram_mod[tokens])\n",
    "\n",
    "id2word = corpora.Dictionary(aux['tokens_bigram'])\n",
    "texts = aux['tokens_bigram'].values\n",
    "\n",
    "corpus = [id2word.doc2bow(text) for text in texts]    \n",
    "\n",
    "def LDA_model(num_topics, passes=1):\n",
    "    return gensim.models.ldamodel.LdaModel(corpus=tqdm_notebook(corpus, leave=False),\n",
    "                                               id2word=id2word,\n",
    "                                               num_topics=num_topics, \n",
    "                                               random_state=100,\n",
    "                                               eval_every=10,\n",
    "                                               chunksize=2000,\n",
    "                                               passes=passes,\n",
    "                                               per_word_topics=True\n",
    "                                            )\n",
    "\n",
    "def compute_coherence(model):\n",
    "    coherence = CoherenceModel(model=model, \n",
    "                           texts=aux['tokens_bigram'].values,\n",
    "                           dictionary=id2word, coherence='c_v')\n",
    "    return coherence.get_coherence()\n",
    "\n",
    "def display_topics(model):\n",
    "    topics = model.show_topics(num_topics=model.num_topics, formatted=False, num_words=10)\n",
    "    topics = map(lambda c: map(lambda cc: cc[0], c[1]), topics)\n",
    "    df = pd.DataFrame(topics)\n",
    "    df.index = ['topic_{0}'.format(i) for i in range(model.num_topics)]\n",
    "    df.columns = ['keyword_{0}'.format(i) for i in range(1, 10+1)]\n",
    "\n",
    "    return df\n",
    "\n",
    "def explore_models(df, rg=range(5, 25)):\n",
    "    models = []\n",
    "    coherences = []\n",
    "    \n",
    "    for num_topics in tqdm_notebook(rg, leave=False):\n",
    "        lda_model = LDA_model(num_topics, passes=5)\n",
    "        models.append(lda_model)\n",
    "        coherence = compute_coherence(lda_model)\n",
    "        coherences.append(coherence)\n",
    "    \n",
    "    return coherences, models\n",
    "\n",
    "coherences, models = explore_models(aux, rg=range(5, 85, 5))\n",
    "\n",
    "best_model = LDA_model(num_topics=5, passes=50)\n",
    "\n",
    "print(display_topics(model=best_model))\n",
    "\n",
    "def get_document_topic_matrix(corpus, num_topics=best_model.num_topics):\n",
    "    matrix = []\n",
    "    for row in tqdm_notebook(corpus):\n",
    "        output = np.zeros(num_topics)\n",
    "        doc_proba = best_model[row][0]\n",
    "        for doc, proba in doc_proba:\n",
    "            output[doc] = proba\n",
    "        matrix.append(output)\n",
    "    matrix = np.array(matrix)\n",
    "    return matrix\n",
    "\n",
    "matrix = get_document_topic_matrix(corpus)\n",
    "\n",
    "doc_topic = best_model.get_document_topics(corpus)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
